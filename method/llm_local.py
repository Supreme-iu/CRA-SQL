import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import StoppingCriteria, StoppingCriteriaList
import warnings
from transformers import logging
from typing import Optional, List, Union

# é…ç½®é™é»˜æ¨¡å¼
warnings.filterwarnings("ignore")
logging.set_verbosity_error()


class LocalLLM:
    def __init__(self, model_path: str = "/model/LLM/DeepSeek-V2-Lite-Chat"):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self._load_model()

    def _load_model(self):
        """æ¨¡å‹åŠ è½½æ–¹æ³•"""
        print(f"ğŸš€ Loading {self.model_path}...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            print("âœ… Model load successfulï¼ (FP16)")
        except Exception as e:
            raise RuntimeError(f"âŒ Load failed: {str(e)}")

    def generate(
            self,
            prompt: str,
            max_tokens: int = 600,
            temperature: float = 0.5,
            top_p: float = 0.9,
            stop: Optional[Union[str, List[str]]] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        Args:
            prompt: è¾“å…¥æ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ§åˆ¶éšæœºæ€§ (0.1-1.0)
            top_p: æ ¸é‡‡æ ·é˜ˆå€¼ (0.5-0.95)
            stop: åœæ­¢è¯ (stræˆ–list), å¦‚ "\n" æˆ– ["###", "</s>"]
        """
        # å‚æ•°æ£€æŸ¥
        assert 0.1 <= temperature <= 1.0, "temperature should be in [0.1, 1.0]"
        assert 0.5 <= top_p <= 0.95, "top_p should be in [0.5, 0.95]"
        if stop is not None:
            stop = [stop] if isinstance(stop, str) else list(stop)

        # æ„å»ºè¾“å…¥
        messages = [{"role": "user", "content": prompt}]
        inputs = self.tokenizer.apply_chat_template(
            messages,
            return_tensors="pt"
        ).to(self.model.device)

        # å®šä¹‰åœæ­¢æ¡ä»¶ç±»
        class StopOnTokens(StoppingCriteria):
            def __init__(self, tokenizer, stop_words):
                self.tokenizer = tokenizer
                # é¢„ç¼–ç æ‰€æœ‰åœæ­¢è¯ï¼ˆå¤„ç†å¤štokenæƒ…å†µï¼‰
                self.stop_token_sequences = [tokenizer.encode(stop, add_special_tokens=False) for stop in stop_words]
                self.max_stop_len = max(len(seq) for seq in self.stop_token_sequences) if stop_words else 0

            def __call__(self, input_ids, scores, **kwargs):
                # æ£€æŸ¥æœ€åNä¸ªtokenï¼ˆN=æœ€é•¿åœæ­¢è¯é•¿åº¦+ç¼“å†²ï¼‰
                check_len = min(32, self.max_stop_len + 4)  # æœ€å¤šæ£€æŸ¥32ä¸ªtoken
                recent_tokens = input_ids[0][-check_len:].tolist()

                # æ£€æŸ¥æ‰€æœ‰åœæ­¢è¯åºåˆ—
                for stop_seq in self.stop_token_sequences:
                    if len(stop_seq) > len(recent_tokens):
                        continue
                    if recent_tokens[-len(stop_seq):] == stop_seq:
                        return True
                return False

        # ç”Ÿæˆé…ç½®
        generate_args = {
            "input_ids": inputs,
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "do_sample": temperature > 0.1,
            "pad_token_id": self.tokenizer.eos_token_id,
            "stopping_criteria": StoppingCriteriaList([StopOnTokens(self.tokenizer, stop)]) if stop else None
        }

        # æ‰§è¡Œç”Ÿæˆ
        outputs = self.model.generate(**generate_args)
        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)

        # åå¤„ç†åœæ­¢è¯ï¼ˆç¡®ä¿å…¼å®¹æ€§ï¼‰
        if stop:
            for stop_word in stop:
                response = response.split(stop_word)[0]
        return response.strip()


# å•ä¾‹æœåŠ¡
llm_service = LocalLLM()


def get_response(
        prompt: str,
        max_tokens: int = 600,
        temperature: float = 0.5,
        top_p: float = 0.9,
        stop: Optional[Union[str, List[str]]] = None
) -> str:
    """
    è·å–æ¨¡å‹å“åº”
    Args:
        stop: æ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨æ ¼å¼ï¼Œä¾‹å¦‚ï¼š
              - stop="\n"          # é‡åˆ°æ¢è¡Œç¬¦åœæ­¢
              - stop=["###", "</s>"] # é‡åˆ°ä»»æ„åœæ­¢è¯åœæ­¢
    """
    return llm_service.generate(
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stop=stop
    )


if __name__ == "__main__":
    response = get_response("ä¸­æ–‡è§£é‡Šä¸€ä¸‹Text2SQLçš„å«ä¹‰")
    print(get_response("æ­å·ç¾é£Ÿ", stop="é±¼"))

    # é‡åˆ°æ¢è¡Œåœæ­¢
    print(get_response("å†™ä¸€é¦–è¯—", stop="\n"))
    print("\nğŸ’¬ Response:")
    print(response)
